{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ovZDjHeEqRf"
   },
   "source": [
    "# Managing Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "M_5fxW5YEqRg"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "import datetime\n",
    "\n",
    "# Libraries for Network creation\n",
    "import torch\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MSELoss\n",
    "from torch.nn import MaxPool2d, Module, BatchNorm2d, Flatten, ConvTranspose2d\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import collections\n",
    "from collections import OrderedDict\n",
    "from collections import Counter\n",
    "\n",
    "# Libraries of models\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets as dataset\n",
    "from torchvision import utils as vutils\n",
    "\n",
    "# make sure you use the GPU (btw check your runtime is a GPU in colab)\n",
    "use_cuda = True\n",
    "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpJ0JIRBEqRi"
   },
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VRHZyXdKeDUh"
   },
   "source": [
    "## Ready dataset import variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "ZXHf2ZUqEqRi"
   },
   "outputs": [],
   "source": [
    "train = 'input/train/train'\n",
    "test = 'input/test/test'\n",
    "destination = test + '/unlabeled/'\n",
    "\n",
    "if not os.path.exists(destination):\n",
    "    os.mkdir(destination)\n",
    "\n",
    "# Move test data to folder\n",
    "file_names = os.listdir(test)\n",
    "for file_name in file_names:\n",
    "    if os.path.exists(os.path.join(test, file_name)):\n",
    "        shutil.move(os.path.join(test, file_name), destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDuE2AGvEqRj"
   },
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fN0HgeMDEqRj",
    "outputId": "3233bed5-1a03-4500-a7fc-55c37e4db6b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full train data size 1700\n",
      "Test data size 3200\n"
     ]
    }
   ],
   "source": [
    "class ImageFolderWithPaths(dataset.ImageFolder):\n",
    "    \"\"\"Custom dataset that includes image file paths. Extends\n",
    "    torchvision.datasets.ImageFolder\n",
    "    \"\"\"\n",
    "\n",
    "    # override the __getitem__ method. this is the method that dataloader calls\n",
    "    def __getitem__(self, index):\n",
    "        # this is what ImageFolder normally returns \n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        # the image file path\n",
    "        path = self.imgs[index][0]\n",
    "        # make a new tuple that includes original and the path\n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "        return tuple_with_path\n",
    "\n",
    "train_data = ImageFolderWithPaths(root=train,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(32),\n",
    "                               transforms.CenterCrop(32),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]),\n",
    "                           ]))\n",
    "print('Full train data size', train_data.__len__())\n",
    "\n",
    "test_data = ImageFolderWithPaths(root=test,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(32),\n",
    "                               transforms.CenterCrop(32),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]),\n",
    "                           ]))\n",
    "print('Test data size', test_data.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGBlVM0EUdwc"
   },
   "source": [
    "## Distribution of images in Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "25dUIvd6UeQ5",
    "outputId": "c6b3a245-9290-45b7-d59c-7c5c7ab3f3c6"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_to_index = train_data.class_to_idx\n",
    "\n",
    "labels_count = {}\n",
    "for key, value in class_to_index.items():\n",
    "  labels_count[key] = dict(Counter(train_data.targets))[value]\n",
    "\n",
    "plt.bar(list(labels_count.keys()), list(labels_count.values()))\n",
    "plt.xlabel('Class Labels')\n",
    "plt.ylabel('Count of Images')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHVzF-i6EqRm"
   },
   "source": [
    "## Create Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 964
    },
    "id": "Ndil91aiEqRm",
    "outputId": "1d3f2ce6-7066-4bcb-bd4c-7935a47a25c8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/mojojojo/opt/anaconda3/lib/python3.8/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/mojojojo/opt/anaconda3/lib/python3.8/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'ImageFolderWithPaths' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/mojojojo/opt/anaconda3/lib/python3.8/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/mojojojo/opt/anaconda3/lib/python3.8/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'ImageFolderWithPaths' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 4453) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py:872\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 872\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    873\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/multiprocessing/queues.py:107\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    106\u001b[0m timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py:66\u001b[0m, in \u001b[0;36m_set_SIGCHLD_handler.<locals>.handler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhandler\u001b[39m(signum, frame):\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# Python can still get and update the process status successfully.\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     \u001b[43m_error_if_any_worker_fails\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m previous_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 4453) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m train_dl \u001b[38;5;241m=\u001b[39m DataLoader(train_data, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Plot some training images\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m real_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m8\u001b[39m))\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py:435\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 435\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    438\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    439\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1068\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1065\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1068\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1071\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1034\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1031\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1033\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1034\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1035\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1036\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py:885\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    884\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[0;32m--> 885\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(pids_str)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    886\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[1;32m    887\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 4453) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "train_dl = DataLoader(train_data, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "# Plot some training images\n",
    "real_batch = next(iter(train_dl))\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))  \n",
    "\n",
    "test_dl = DataLoader(test_data, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "# Plot some test images\n",
    "real_batch = next(iter(test_dl))\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Testing Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xhas3uebEqRm"
   },
   "source": [
    "# Prepare Simple Convolutional Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e31qJaked9s4"
   },
   "source": [
    "## Creating the network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "MyfcPDeIEqRm"
   },
   "outputs": [],
   "source": [
    "model = Sequential(OrderedDict([ # -> 3, 32, 32\n",
    "          ('conv1', Conv2d(3,16,3,padding=1)),  \n",
    "          ('normalise1', BatchNorm2d(16)),\n",
    "          ('relu1', ReLU()),\n",
    "          ('conv2', Conv2d(16,32,3,padding=1)), \n",
    "          ('relu2', ReLU()),\n",
    "          ('conv3', Conv2d(32,64,3,padding=1)), \n",
    "          ('relu3', ReLU()),\n",
    "          ('maxpool1', MaxPool2d(kernel_size=2)),\n",
    "          ('flatten', Flatten()),\n",
    "          ('lin1', Linear(64*16*16, 1024)),\n",
    "          ('relu4', ReLU()),\n",
    "          ('lin2', Linear(1024, 4))\n",
    "        ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjqEviuCEqRn"
   },
   "source": [
    "## Methods to train and test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "IXa9KObAEqRn"
   },
   "outputs": [],
   "source": [
    "epoch_print_gap = 1\n",
    "\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels, image_name in train_loader:\n",
    "            outputs = model(imgs.to(device))\n",
    "            loss = loss_fn(outputs, labels.to(device))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_train += loss.item()\n",
    "            \n",
    "        if epoch == 1 or epoch % epoch_print_gap == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "                datetime.datetime.now(), epoch, float(loss_train)))\n",
    "\n",
    "\n",
    "def test_loop(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target, image_name in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRSpURFUYV7h"
   },
   "source": [
    "## Training and testing the model on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CuIL7_ZKEqRo",
    "outputId": "397b0c5e-4515-4ad8-bb0c-5ef822001df5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-25 22:39:41.088073 Epoch 1, Training loss 18.144352436065674\n",
      "2022-03-25 22:39:52.122772 Epoch 2, Training loss 15.920716881752014\n",
      "2022-03-25 22:40:04.810365 Epoch 3, Training loss 12.934784710407257\n",
      "2022-03-25 22:40:15.820385 Epoch 4, Training loss 10.18836522102356\n",
      "2022-03-25 22:40:26.695393 Epoch 5, Training loss 8.62924736738205\n",
      "2022-03-25 22:40:42.230560 Epoch 6, Training loss 7.744423598051071\n",
      "2022-03-25 22:40:53.308261 Epoch 7, Training loss 7.142036944627762\n",
      "2022-03-25 22:41:05.353584 Epoch 8, Training loss 6.551529258489609\n",
      "2022-03-25 22:41:17.200136 Epoch 9, Training loss 6.2242125272750854\n",
      "2022-03-25 22:41:28.303807 Epoch 10, Training loss 5.805073082447052\n",
      "\n",
      "Test set: Average loss: -2.9811, Accuracy: 1450/1700 (85%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# optimizer = Adam(model.parameters())\n",
    "optimizer = SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "loss_fn = CrossEntropyLoss(weight=torch.FloatTensor([0.7, 0.7, 0.85, 1.00]))\n",
    "model = model.to(device)\n",
    "\n",
    "n_epochs = 10 \n",
    "\n",
    "# train the CNN\n",
    "training_loop(\n",
    "    n_epochs = n_epochs, \n",
    "    optimizer = optimizer,\n",
    "    model = model, \n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_dl\n",
    ")\n",
    "\n",
    "test_loop(model = model, test_loader = train_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ePxJlqtwYeYK"
   },
   "source": [
    "## Predict class for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "tx3nvsWUEqRo"
   },
   "outputs": [],
   "source": [
    "def extract_image_name(filenames):\n",
    "  # Sample filename - 'kaggle/input/deep-learning-for-msc-coursework-2022/test/test/unlabeled/tile5001.png'\n",
    "  names = []\n",
    "  for name in filenames:\n",
    "    names.append(name.split('/')[-1])\n",
    "  return names\n",
    "\n",
    "def transform_to_class(predicted):\n",
    "  labels = []\n",
    "  for pred in predicted:\n",
    "    val = pred[0]\n",
    "    for key, value in class_to_index.items():\n",
    "      if val == value:\n",
    "        labels.append(key)\n",
    "  return labels\n",
    "\n",
    "def predict(model, test_loader):\n",
    "  model.eval()    \n",
    "  images, preds = [], []\n",
    "  with torch.no_grad():\n",
    "    for data, target, image_name in test_loader:\n",
    "      data = data.to(device)\n",
    "      output = model(data)\n",
    "      pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "\n",
    "      preds = preds + transform_to_class(pred.detach().numpy())\n",
    "      images = images + extract_image_name(image_name)\n",
    "\n",
    "  return preds, images\n",
    "\n",
    "preds, images = predict(model, test_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7cGUaeKYnkA"
   },
   "source": [
    "## Save results to test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "so49faIaEqRp",
    "outputId": "ee8a78fa-e70b-4536-8e43-012f3de5411f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cancer 1032\n",
      "Immune 676\n",
      "Connective 584\n",
      "Normal 908\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-0dc63438-cf14-4217-91a3-04430eb047ad\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tile6364.png</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tile5040.png</td>\n",
       "      <td>Connective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tile5681.png</td>\n",
       "      <td>Connective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tile5071.png</td>\n",
       "      <td>Cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tile6218.png</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3195</th>\n",
       "      <td>tile7300.png</td>\n",
       "      <td>Cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3196</th>\n",
       "      <td>tile5017.png</td>\n",
       "      <td>Cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3197</th>\n",
       "      <td>tile7122.png</td>\n",
       "      <td>Cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3198</th>\n",
       "      <td>tile5165.png</td>\n",
       "      <td>Cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3199</th>\n",
       "      <td>tile7490.png</td>\n",
       "      <td>Immune</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3200 rows × 2 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0dc63438-cf14-4217-91a3-04430eb047ad')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-0dc63438-cf14-4217-91a3-04430eb047ad button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-0dc63438-cf14-4217-91a3-04430eb047ad');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                Id        Type\n",
       "0     tile6364.png      Normal\n",
       "1     tile5040.png  Connective\n",
       "2     tile5681.png  Connective\n",
       "3     tile5071.png      Cancer\n",
       "4     tile6218.png      Normal\n",
       "...            ...         ...\n",
       "3195  tile7300.png      Cancer\n",
       "3196  tile5017.png      Cancer\n",
       "3197  tile7122.png      Cancer\n",
       "3198  tile5165.png      Cancer\n",
       "3199  tile7490.png      Immune\n",
       "\n",
       "[3200 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Cancer', preds.count('Cancer'))\n",
    "print('Immune', preds.count('Immune'))\n",
    "print('Connective', preds.count('Connective'))\n",
    "print('Normal', preds.count('Normal'))\n",
    "\n",
    "results_df = pd.DataFrame({'Id': images, 'Type': preds})\n",
    "display(results_df)\n",
    "results_df.to_csv('simple_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "schxaSGUYrWa"
   },
   "source": [
    "# Prepare Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IwR-6If4dSox"
   },
   "source": [
    "## Creating the encoder, decoder and autoencoder class definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "jcrSoACXIyEm"
   },
   "outputs": [],
   "source": [
    "class Encoder(Module):\n",
    "  def __init__(self):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.cnn_encoder = Sequential(OrderedDict([\n",
    "        ('conv1', Conv2d(3,32,3,padding=1)), \n",
    "        ('relu1', ReLU()),\n",
    "        ('conv2', Conv2d(32,64,3,padding=1)), \n",
    "        ('relu2', ReLU()),\n",
    "        ('maxpool1',MaxPool2d(2))\n",
    "      ]))\n",
    "        \n",
    "  def forward(self, x):\n",
    "    return self.cnn_encoder(x)\n",
    "\n",
    "class Decoder(Module):\n",
    "  def __init__(self):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.cnn_decoder = Sequential(OrderedDict([\n",
    "      ('transpose1', ConvTranspose2d(64,32,3,stride=2,output_padding=1)),\n",
    "      ('relu1', ReLU()),\n",
    "      ('transpose2', ConvTranspose2d(32,3,3,padding=2)),\n",
    "      ('relu2', ReLU()),\n",
    "    ]))\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.cnn_decoder(x)\n",
    "    x = torch.sigmoid(x)\n",
    "    return x\n",
    "\n",
    "class Autoencoder(Module):\n",
    "  def __init__(self):\n",
    "    super(Autoencoder, self).__init__()\n",
    "    self.encoder = Encoder()\n",
    "    self.decoder = Decoder()\n",
    "\n",
    "  def forward(self, x):\n",
    "    y = self.encoder(x)\n",
    "    y = self.decoder(y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXS-Rt6uAzjv"
   },
   "source": [
    "## Method to train autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "jtb365LGA3uC"
   },
   "outputs": [],
   "source": [
    "epoch_print_gap = 1\n",
    "\n",
    "def train_autoencoder(model, train_loader, n_epochs, loss_fn, optimizer):\n",
    "    # Set train mode for both the encoder and the decoder\n",
    "    model.encoder.train()\n",
    "    model.decoder.train()\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "      loss_train = 0.0\n",
    "      for imgs, labels, image_name in train_loader:\n",
    "        encoded_output = model.encoder(imgs)\n",
    "        decoded_output = model.decoder(encoded_output)\n",
    "        \n",
    "        loss = loss_fn(decoded_output, imgs)\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_train += loss.item()\n",
    "            \n",
    "      if epoch == 1 or epoch % epoch_print_gap == 0:\n",
    "        print('{} Epoch {}, Training loss {}'.format(\n",
    "            datetime.datetime.now(), epoch, float(loss_train)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2clqidE3CzHR"
   },
   "source": [
    "## Training autoencoder on unlabelled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sjUABN69Cyyb",
    "outputId": "5a7dfa47-787a-4127-e9b3-975418c52f7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-25 22:42:02.314778 Epoch 1, Training loss 29.34079098701477\n",
      "2022-03-25 22:42:19.816240 Epoch 2, Training loss 26.141397714614868\n",
      "2022-03-25 22:42:35.769346 Epoch 3, Training loss 22.842299818992615\n",
      "2022-03-25 22:42:53.847684 Epoch 4, Training loss 20.792163848876953\n",
      "2022-03-25 22:43:09.491278 Epoch 5, Training loss 19.12102872133255\n",
      "2022-03-25 22:43:27.308331 Epoch 6, Training loss 18.768859803676605\n",
      "2022-03-25 22:43:43.119526 Epoch 7, Training loss 18.629883289337158\n",
      "2022-03-25 22:44:00.818283 Epoch 8, Training loss 18.543194353580475\n",
      "2022-03-25 22:44:16.518595 Epoch 9, Training loss 18.476490199565887\n",
      "2022-03-25 22:44:35.277718 Epoch 10, Training loss 18.418786644935608\n",
      "2022-03-25 22:44:59.715566 Epoch 11, Training loss 18.365592181682587\n",
      "2022-03-25 22:45:15.671595 Epoch 12, Training loss 18.314928889274597\n",
      "2022-03-25 22:45:33.580503 Epoch 13, Training loss 18.266565561294556\n",
      "2022-03-25 22:45:49.414373 Epoch 14, Training loss 18.22133082151413\n",
      "2022-03-25 22:46:06.882534 Epoch 15, Training loss 18.17810183763504\n",
      "2022-03-25 22:46:22.685302 Epoch 16, Training loss 18.136042952537537\n",
      "2022-03-25 22:46:40.754868 Epoch 17, Training loss 18.094722151756287\n",
      "2022-03-25 22:46:56.485925 Epoch 18, Training loss 18.054012894630432\n",
      "2022-03-25 22:47:14.016075 Epoch 19, Training loss 18.013742327690125\n",
      "2022-03-25 22:47:29.944411 Epoch 20, Training loss 17.971591770648956\n"
     ]
    }
   ],
   "source": [
    "autoencoder = Autoencoder().to(device)\n",
    "\n",
    "loss_fn = MSELoss()\n",
    "\n",
    "optimizer = SGD(autoencoder.parameters(), lr=0.01)\n",
    "\n",
    "n_epochs = 20\n",
    "\n",
    "train_autoencoder(autoencoder, test_dl, n_epochs, loss_fn, optimizer)\n",
    "\n",
    "torch.save(autoencoder.encoder.state_dict(), 'AutoEncoder_encoder_state.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5g6cEUs3l8_i"
   },
   "source": [
    "# Prepare Encoder based Convolutional Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aCHXGG0amDE4"
   },
   "source": [
    "## Create the network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RJ93uQc4mCnL",
    "outputId": "d4f7fca1-a2e4-4499-a637-486d96eb01d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_model = Sequential(OrderedDict([\n",
    "                 ('conv1', Conv2d(64,64,3,padding=1)),\n",
    "                 ('relu1', ReLU()),\n",
    "                 ('conv2', Conv2d(64,128,3,padding=1)),\n",
    "                 ('relu2', ReLU()),\n",
    "                 ('conv3', Conv2d(128,256,3,padding=1)),\n",
    "                 ('relu3', ReLU()),\n",
    "                 ('flatten', Flatten()),\n",
    "                 ('lin1', Linear(256*16*16, 2048)),\n",
    "                 ('relu4', ReLU()),\n",
    "                 ('lin2', Linear(2048,4))\n",
    "              ]))\n",
    "\n",
    "encoder = Encoder()\n",
    "encoder.load_state_dict(torch.load('AutoEncoder_encoder_state.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uP2yAFdHnGfe"
   },
   "source": [
    "## Method to train and test encoder CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "pyvRehd8nJqq"
   },
   "outputs": [],
   "source": [
    "epoch_print_gap = 1\n",
    "\n",
    "def train_encoder_model(n_epochs, optimizer, model, encoder, loss_fn, train_loader):\n",
    "    model.train() \n",
    "    encoder.eval()\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels, image_name in train_loader:\n",
    "            encoded_output = encoder(imgs)\n",
    "            outputs = model(encoded_output)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_train += loss.item()\n",
    "            \n",
    "        if epoch == 1 or epoch % epoch_print_gap == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "                datetime.datetime.now(), epoch, float(loss_train)))\n",
    "\n",
    "\n",
    "def test_encoder_model(model, encoder, test_loader):\n",
    "    model.eval()\n",
    "    encoder.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target, image_name in test_loader:\n",
    "            encoded_output = encoder(data)\n",
    "            outputs = model(encoded_output)\n",
    "            test_loss += F.nll_loss(outputs, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = outputs.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHRRKSHQn1Ak"
   },
   "source": [
    "## Training and testing the encoded CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "91Anl0sEn1d6",
    "outputId": "69f0ec7c-bff2-43fc-b4ec-6e427285e7bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-25 22:48:31.914636 Epoch 1, Training loss 18.47675085067749\n",
      "2022-03-25 22:49:25.937062 Epoch 2, Training loss 16.460161566734314\n",
      "2022-03-25 22:50:15.336560 Epoch 3, Training loss 14.730190753936768\n",
      "2022-03-25 22:51:08.721540 Epoch 4, Training loss 14.165429055690765\n",
      "2022-03-25 22:52:00.078304 Epoch 5, Training loss 12.870494902133942\n",
      "2022-03-25 22:52:51.236199 Epoch 6, Training loss 11.659909546375275\n",
      "2022-03-25 22:53:42.379447 Epoch 7, Training loss 11.141396164894104\n",
      "2022-03-25 22:54:32.039235 Epoch 8, Training loss 11.098358929157257\n",
      "2022-03-25 22:55:25.024235 Epoch 9, Training loss 11.60904175043106\n",
      "2022-03-25 22:56:24.004799 Epoch 10, Training loss 10.745695650577545\n",
      "2022-03-25 22:57:15.679994 Epoch 11, Training loss 11.228126645088196\n",
      "2022-03-25 22:58:05.563867 Epoch 12, Training loss 11.096832036972046\n",
      "2022-03-25 22:58:55.018250 Epoch 13, Training loss 9.879035830497742\n",
      "2022-03-25 22:59:44.938710 Epoch 14, Training loss 11.581725895404816\n",
      "2022-03-25 23:00:34.331510 Epoch 15, Training loss 9.876316964626312\n",
      "2022-03-25 23:01:23.923394 Epoch 16, Training loss 9.756659209728241\n"
     ]
    }
   ],
   "source": [
    "optimizer = SGD(encode_model.parameters(), lr=0.01)\n",
    "\n",
    "loss_fn = CrossEntropyLoss(weight=torch.FloatTensor([0.7, 0.7, 0.85, 1.00]))\n",
    "\n",
    "n_epochs = 20 \n",
    "\n",
    "# train the CNN\n",
    "train_encoder_model(\n",
    "    n_epochs = n_epochs, \n",
    "    optimizer = optimizer,\n",
    "    model = encode_model, \n",
    "    encoder = encoder,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_dl\n",
    ")\n",
    "\n",
    "test_encoder_model(model=encode_model, encoder=encoder, test_loader=train_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRoazddzn13c"
   },
   "source": [
    "## Predict class for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "srXwZNGOn2VI"
   },
   "outputs": [],
   "source": [
    "def predict_encoded_model(model, encoder, test_loader):\n",
    "  model.eval() \n",
    "  encoder.eval()   \n",
    "  images, preds = [], []\n",
    "  with torch.no_grad():\n",
    "    for data, target, image_name in test_loader:\n",
    "      encoded_output = encoder(data)\n",
    "      output = model(encoded_output)\n",
    "      pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "\n",
    "      preds = preds + transform_to_class(pred.detach().numpy())\n",
    "      images = images + extract_image_name(image_name)\n",
    "\n",
    "  return preds, images\n",
    "\n",
    "preds, images = predict_encoded_model(encode_model, encoder, test_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OkmPP5gpFFJ"
   },
   "source": [
    "## Save results to test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_euZrV1PpEzU"
   },
   "outputs": [],
   "source": [
    "print('Cancer', preds.count('Cancer'))\n",
    "print('Immune', preds.count('Immune'))\n",
    "print('Connective', preds.count('Connective'))\n",
    "print('Normal', preds.count('Normal'))\n",
    "\n",
    "results_df = pd.DataFrame({'Id': images, 'Type': preds})\n",
    "display(results_df)\n",
    "results_df.to_csv('encoder_test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DL Coursework.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
